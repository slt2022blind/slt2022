---
layout: distill
title: GAN You Hear Me? <br/> Reclaiming unconditional speech synthesis from diffusion models
description: Demo website for the submitted paper to SLT 2022
date: 2022-07-02
permalink: /

authors:
  - name: BLIND
    # url: "https://en.wikipedia.org/wiki/Albert_Einstein"
    affiliations:
      name: BLIND
  # - name: Boris Podolsky
  #   url: "https://en.wikipedia.org/wiki/Boris_Podolsky"
  #   affiliations:
  #     name: IAS, Princeton

bibliography: #2018-12-22-distill.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Introduction
    # if a section has subsections, you can add them as follows:
    # subsections:
    #   - name: Example Child Subsection 1
    #   - name: Example Child Subsection 2
  - name: Code & Pretrained models
  - name: Unconditional samples
  - name: Unseen tasks
    subsections:
      - name: Voice conversion
      - name: Digit conversion
      - name: Speech enhancement
      - name: Bonus
  - name: BLIND

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
  .audio-smaller {
    width: 10vw;
  }
  .video-medium {
    width: 20vw;
  }

  audio::-webkit-media-controls-current-time-display,
  audio::-webkit-media-controls-time-remaining-display {
      display: none;
  }
  .table-position {
    max-width: 90vw;
    margin: auto;
    text-align: center;
  }

---

## Introduction

This is the demonstration website for the paper submitted for SLT 2022.
It contains generated audio samples from both our ASGAN model and the baseline implementations against which we compare.

## Code & Pretrained models

BLIND FOR PEER REVIEW.

(Links and website will be updated upon publication)

***

## Unconditional samples

<!-- Table of generated samples for each model -->

Below is a table of unconditional samples generated form each model using the same random seed (for `torch`, `numpy`, and base `python`):

{: .l-screen .table-position}

| Sample        | WaveGAN | SaShiMi  | DiffWave | SaShiMi+DiffWave | ASGAN (mel-spec ver.) | ASGAN (HuBERT ver.) |
| ------------- |:--:| :--: | :--: | :--: | :--: | :--: |{% for sind in (0..9) -%}
{% assign h = sind %}{% if h < 10 -%}{% assign h = h | prepend: '0' %}{% endif %}
| {{ h }} | ![]({{ site.baseurl }}/assets/img/wavegan_sc09/gen-{{ h }}.jpg){: width="70%"}  <audio controls controlsList="nodownload noplaybackrate" class='audio-smaller'><source src="{{ site.baseurl }}/assets/img/wavegan_sc09/gen-{{h}}.wav"></audio> | ![]({{ site.baseurl }}/assets/img/sashimi_ar_sc09/gen-{{ h }}.jpg){: width="70%"}  <audio controls controlsList="nodownload noplaybackrate" class='audio-smaller'><source src="{{ site.baseurl }}/assets/img/sashimi_ar_sc09/gen-{{h}}.wav"></audio>  |  ![]({{ site.baseurl }}/assets/img/diffwave_sc09/gen-{{ h }}.jpg){: width="70%"}  <audio controls controlsList="nodownload noplaybackrate" class='audio-smaller'><source src="{{ site.baseurl }}/assets/img/diffwave_sc09/gen-{{h}}.wav"></audio>  |  ![]({{ site.baseurl }}/assets/img/sashimi_diffwave_800k_sc09/gen-{{ h }}.jpg){: width="70%"}  <audio controls controlsList="nodownload noplaybackrate" class='audio-smaller'><source src="{{ site.baseurl }}/assets/img/sashimi_diffwave_800k_sc09/gen-{{h}}.wav"></audio> |  ![]({{ site.baseurl }}/assets/img/conv2_sc09_mel5v2/gen-{{ h }}.jpg){: width="70%"}  <audio controls controlsList="nodownload noplaybackrate" class='audio-smaller'><source src="{{ site.baseurl }}/assets/img/conv2_sc09_mel5v2/gen-{{h}}.wav"></audio> | ![]({{ site.baseurl }}/assets/img/conv2_sc09_6/gen-{{ h }}.jpg){: width="70%"}  <audio controls controlsList="nodownload noplaybackrate" class='audio-smaller'><source src="{{ site.baseurl }}/assets/img/conv2_sc09_6/gen-{{h}}.wav"></audio>  |{% endfor %}


## Unseen tasks

In the following, we show a few examples of using ASGAN on unseen tasks.
To reiterate as in the paper, we define the *course styles* as those $$ \mathbf{w} $$ which are fed as input to the first 10 layers (first two `Style Block` groups) and the input to the base Fourier Feature layer.
The *fine styles* are those $$ \mathbf{w} $$ which are used as inputs to the last 5 modulated convolution layers (last two `Style Block` groups).

### Voice conversion

To perform voice conversion *from* a speech feature sequence $$X_1$$ *to* a speaker from another utterance's features $$X_2$$, start with the projection of $$X_1$$ (i.e. $$\mathbf{w}_1$$) and then interpolate the **fine** styles between $$\mathbf{w}_1$$ and $$\mathbf{w}_2$$ to gradually change the speaker identity.

Several examples are given below (the first of which is the same example from the paper) using a truncation $$\psi=0.3$$:

{: .l-screen .table-position}

| Sample | Projected $$X_1$$ | Projected $$X_2$$  | Course styles: $$\mathbf{w}_1 $$<br>Fine styles: $$\mathbf{w}_1 + 0.5(\mathbf{w}_2 - \mathbf{w}_1)$$ | Course styles: $$\mathbf{w}_1$$<br>Fine styles: $$\mathbf{w}_1 + 1.0(\mathbf{w}_2 - \mathbf{w}_1)$$  | Course styles: $$\mathbf{w}_1$$<br>Fine styles: $$\mathbf{w}_1 + 1.5(\mathbf{w}_2 - \mathbf{w}_1)$$ | Course styles: $$\mathbf{w}_1$$<br>Fine styles: $$\mathbf{w}_1 + 2.0(\mathbf{w}_2 - \mathbf{w}_1)$$  |
| ------------- |:--:| :--: | :--: | :--: | :--: | :--: |{% for sind in (0..5) -%}
{% assign h = sind %}{% if h < 10 -%}{% assign h = h | prepend: '0' %}{% endif %}
| {{ h }} | ![]({{ site.baseurl }}/assets/img/vc/{{h}}/fine_interp_X1.jpg){: width="70%"}  <audio controls controlsList="nodownload noplaybackrate" class='audio-smaller'><source src="{{ site.baseurl }}/assets/img/vc/{{h}}/fine_interp_X1.wav"></audio> | ![]({{ site.baseurl }}/assets/img/vc/{{h}}/fine_interp_X2.jpg){: width="70%"}  <audio controls controlsList="nodownload noplaybackrate" class='audio-smaller'><source src="{{ site.baseurl }}/assets/img/vc/{{h}}/fine_interp_X2.wav"></audio>  |  ![]({{ site.baseurl }}/assets/img/vc/{{h}}/fine_interp_0.5.jpg){: width="70%"}  <audio controls controlsList="nodownload noplaybackrate" class='audio-smaller'><source src="{{ site.baseurl }}/assets/img/vc/{{h}}/fine_interp_0.5.wav"></audio>  |  ![]({{ site.baseurl }}/assets/img/vc/{{h}}/fine_interp_1.0.jpg){: width="70%"}  <audio controls controlsList="nodownload noplaybackrate" class='audio-smaller'><source src="{{ site.baseurl }}/assets/img/vc/{{h}}/fine_interp_1.0.wav"></audio> |  ![]({{ site.baseurl }}/assets/img/vc/{{h}}/fine_interp_1.5.jpg){: width="70%"}  <audio controls controlsList="nodownload noplaybackrate" class='audio-smaller'><source src="{{ site.baseurl }}/assets/img/vc/{{h}}/fine_interp_1.5.wav"></audio> | ![]({{ site.baseurl }}/assets/img/vc/{{h}}/fine_interp_2.0.jpg){: width="70%"}  <audio controls controlsList="nodownload noplaybackrate" class='audio-smaller'><source src="{{ site.baseurl }}/assets/img/vc/{{h}}/fine_interp_2.0.wav"></audio>  |{% endfor %}
 

### Digit conversion

To perform digit conversion (i.e. speech editing the transcript) *from* a speech feature sequence $$X_1$$ *to* the digit spoken in another utterance's features $$X_2$$, we start with the projection of $$X_1$$ (i.e. $$\mathbf{w}_1$$) and then interpolate the **course** styles between $$\mathbf{w}_1$$ and $$\mathbf{w}_2$$ to gradually change the speaker identity.

Several examples are given below (the first of which is the same example from the paper) using a truncation $$\psi=0.3$$:

{: .l-screen .table-position}

| Sample | Projected $$X_1$$ | Projected $$X_2$$  | Course styles: $$\mathbf{w}_1 + 0.5(\mathbf{w}_2 - \mathbf{w}_1)$$<br>Fine styles: $$\mathbf{w}_1 $$ | Course styles: $$\mathbf{w}_1 + 0.8(\mathbf{w}_2 - \mathbf{w}_1)$$<br>Fine styles: $$\mathbf{w}_1$$ | Course styles: $$\mathbf{w}_1 + 1.5(\mathbf{w}_2 - \mathbf{w}_1)$$<br>Fine styles: $$\mathbf{w}_1$$  | Course styles: $$\mathbf{w}_1 + 2.0(\mathbf{w}_2 - \mathbf{w}_1)$$<br>Fine styles: $$\mathbf{w}_1$$ |
| ------------- |:--:| :--: | :--: | :--: | :--: | :--: |{% for sind in (0..5) -%}
{% assign h = sind %}{% if h < 10 -%}{% assign h = h | prepend: '0' %}{% endif %}
| {{ h }} | ![]({{ site.baseurl }}/assets/img/dc/{{h}}/course_interp_X1.jpg){: width="70%"}  <audio controls controlsList="nodownload noplaybackrate" class='audio-smaller'><source src="{{ site.baseurl }}/assets/img/dc/{{h}}/course_interp_X1.wav"></audio> | ![]({{ site.baseurl }}/assets/img/dc/{{h}}/course_interp_X2.jpg){: width="70%"}  <audio controls controlsList="nodownload noplaybackrate" class='audio-smaller'><source src="{{ site.baseurl }}/assets/img/dc/{{h}}/course_interp_X2.wav"></audio>  |  ![]({{ site.baseurl }}/assets/img/dc/{{h}}/course_interp_0.5.jpg){: width="70%"}  <audio controls controlsList="nodownload noplaybackrate" class='audio-smaller'><source src="{{ site.baseurl }}/assets/img/dc/{{h}}/course_interp_0.5.wav"></audio>  |  ![]({{ site.baseurl }}/assets/img/dc/{{h}}/course_interp_0.8.jpg){: width="70%"}  <audio controls controlsList="nodownload noplaybackrate" class='audio-smaller'><source src="{{ site.baseurl }}/assets/img/dc/{{h}}/course_interp_0.8.wav"></audio> |  ![]({{ site.baseurl }}/assets/img/dc/{{h}}/course_interp_0.95.jpg){: width="70%"}  <audio controls controlsList="nodownload noplaybackrate" class='audio-smaller'><source src="{{ site.baseurl }}/assets/img/dc/{{h}}/course_interp_0.95.wav"></audio> | ![]({{ site.baseurl }}/assets/img/dc/{{h}}/course_interp_1.0.jpg){: width="70%"}  <audio controls controlsList="nodownload noplaybackrate" class='audio-smaller'><source src="{{ site.baseurl }}/assets/img/dc/{{h}}/course_interp_1.0.wav"></audio>  |{% endfor %}

### Speech enhancement

For speech enhancement we follow the procedure outlined in the paper, using $$N=10$$ and $$\sigma^2=0.001$$.
Concretely, after projection, we move in the direction of the decreasing noise vector $$\mathbf{\delta}$$ (or in the opposite direction to increase noise).

Several examples are given below (the first of which is the same as in the paper), where all the samples in the left column are particularly noisy utterances found in the SC09 test set:

{: .l-screen .table-position}

| Sample | Raw $$X_0$$ | Projection $$\tilde{X}_0$$  | All styles: $$\mathbf{w}_0 -3 \mathbf{\delta}$$ | All styles: $$\mathbf{w}_0 + 3\mathbf{\delta}$$ | All styles: $$\mathbf{w}_0 + 6\mathbf{\delta}$$ | All styles: $$\mathbf{w}_0 + 9.0\mathbf{\delta}$$ |
| ------------- |:--:| :--: | :--: | :--: | :--: | :--: |{% for sind in (0..5) -%}
{% assign h = sind %}{% if h < 10 -%}{% assign h = h | prepend: '0' %}{% endif %}
| {{ h }} | ![]({{ site.baseurl }}/assets/img/se/{{h}}/se_interp_X1proj.jpg){: width="70%"}  <audio controls controlsList="nodownload noplaybackrate" class='audio-smaller'><source src="{{ site.baseurl }}/assets/img/se/{{h}}/se_interp_X1proj.wav"></audio> | ![]({{ site.baseurl }}/assets/img/se/{{h}}/se_interp_X1orig.jpg){: width="70%"}  <audio controls controlsList="nodownload noplaybackrate" class='audio-smaller'><source src="{{ site.baseurl }}/assets/img/se/{{h}}/se_interp_X1orig.wav"></audio>  |  ![]({{ site.baseurl }}/assets/img/se/{{h}}/se_interp_-3.jpg){: width="70%"}  <audio controls controlsList="nodownload noplaybackrate" class='audio-smaller'><source src="{{ site.baseurl }}/assets/img/se/{{h}}/se_interp_-3.wav"></audio>  |  ![]({{ site.baseurl }}/assets/img/se/{{h}}/se_interp_3.jpg){: width="70%"}  <audio controls controlsList="nodownload noplaybackrate" class='audio-smaller'><source src="{{ site.baseurl }}/assets/img/se/{{h}}/se_interp_3.wav"></audio> |  ![]({{ site.baseurl }}/assets/img/se/{{h}}/se_interp_6.jpg){: width="70%"}  <audio controls controlsList="nodownload noplaybackrate" class='audio-smaller'><source src="{{ site.baseurl }}/assets/img/se/{{h}}/se_interp_6.wav"></audio> | ![]({{ site.baseurl }}/assets/img/se/{{h}}/se_interp_9.jpg){: width="70%"}  <audio controls controlsList="nodownload noplaybackrate" class='audio-smaller'><source src="{{ site.baseurl }}/assets/img/se/{{h}}/se_interp_9.wav"></audio>  |{% endfor %}

***

### Bonus

#### Bonus 1: Out of domain sample

As a bonus, here is what happens when attempting to project a extreme out-of-domain sample -- one of the authors saying the "SLT" (a word/acronym not even remotely similar to the dataset)!
After projection, we try to apply voice conversion to this extreme out of domain sample $$X_1$$.
To get this to work, we had to modify the inversion technique by using 3000 updates instead of 1000.

{: .l-screen .table-position}

| Original waveform<br>$$X_1$$ | Projected $$X_1$$ | Projected $$X_2$$  | Course styles: $$\mathbf{w}_1 $$<br>Fine styles: $$\mathbf{w}_1 + 0.5(\mathbf{w}_2 - \mathbf{w}_1)$$ | Course styles: $$\mathbf{w}_1$$<br>Fine styles: $$\mathbf{w}_1 + 1.0(\mathbf{w}_2 - \mathbf{w}_1)$$  | Course styles: $$\mathbf{w}_1$$<br>Fine styles: $$\mathbf{w}_1 + 1.5(\mathbf{w}_2 - \mathbf{w}_1)$$ | Course styles: $$\mathbf{w}_1$$<br>Fine styles: $$\mathbf{w}_1 + 2.0(\mathbf{w}_2 - \mathbf{w}_1)$$  |
| :-------------: |:--:| :--: | :--: | :--: | :--: | :--: |
| ![]({{ site.baseurl }}/assets/img/bonus/out_of_domain/original.jpg){: width="70%"}  <audio controls controlsList="nodownload noplaybackrate" class='audio-smaller'><source src="{{ site.baseurl }}/assets/img/bonus/out_of_domain/original.wav"></audio> | ![]({{ site.baseurl }}/assets/img/bonus/out_of_domain/fine_interp_X1.jpg){: width="70%"}  <audio controls controlsList="nodownload noplaybackrate" class='audio-smaller'><source src="{{ site.baseurl }}/assets/img/bonus/out_of_domain/fine_interp_X1.wav"></audio> | ![]({{ site.baseurl }}/assets/img/bonus/out_of_domain/fine_interp_X2.jpg){: width="70%"}  <audio controls controlsList="nodownload noplaybackrate" class='audio-smaller'><source src="{{ site.baseurl }}/assets/img/bonus/out_of_domain/fine_interp_X2.wav"></audio>  |  ![]({{ site.baseurl }}/assets/img/bonus/out_of_domain/fine_interp_0.5.jpg){: width="70%"}  <audio controls controlsList="nodownload noplaybackrate" class='audio-smaller'><source src="{{ site.baseurl }}/assets/img/bonus/out_of_domain/fine_interp_0.5.wav"></audio>  |  ![]({{ site.baseurl }}/assets/img/bonus/out_of_domain/fine_interp_1.0.jpg){: width="70%"}  <audio controls controlsList="nodownload noplaybackrate" class='audio-smaller'><source src="{{ site.baseurl }}/assets/img/bonus/out_of_domain/fine_interp_1.0.wav"></audio> |  ![]({{ site.baseurl }}/assets/img/bonus/out_of_domain/fine_interp_1.5.jpg){: width="70%"}  <audio controls controlsList="nodownload noplaybackrate" class='audio-smaller'><source src="{{ site.baseurl }}/assets/img/bonus/out_of_domain/fine_interp_1.5.wav"></audio> | ![]({{ site.baseurl }}/assets/img/bonus/out_of_domain/fine_interp_2.0.jpg){: width="70%"}  <audio controls controlsList="nodownload noplaybackrate" class='audio-smaller'><source src="{{ site.baseurl }}/assets/img/bonus/out_of_domain/fine_interp_2.0.wav"></audio>  |

<br/>

As we can see, it doesn't perform voice conversion very well! 
We suspect this is because the projected point $$\mathbf{w}_1$$ is so far out-of-domain that it lies far away from the manifold upon which the training data lies within the $$W$$-space.
Intuitively, we hypothesize that because $$\mathbf{w}_1$$ is so far out of domain, the part of the $$W$$-space it inhabits is no longer linearly disentangled as it is so far away from the other points. 
Since things are no longer linearly disentangled, linear operations we perform to try and achieve voice conversion fail in strange ways, as seen above.

#### Bonus 2: Latent interpolation videos

To give a final visual indication of the disentanglement of the latent space, we perform a walk within the latent space between several sampled $$\mathbf{w}$$ vectors to produce a video similar to those typically shown for unconditional image synthesis models.
For the videos below we use a truncation $$\psi=0.5$$ and linearly interpolate within the $$W$$-space between several anchor points:

{: .l-screen .table-position}

| Example 1 | Example 2 | Example 3 |
| :--: |:--:| :--: | 
| <video class='video-medium' controls autoplay muted><source src="{{ site.baseurl }}/assets/img/bonus/w_interp1.mp4" type="video/mp4"></video> | <video class='video-medium' controls autoplay muted><source src="{{ site.baseurl }}/assets/img/bonus/w_interp2.mp4" type="video/mp4"></video> | <video class='video-medium' controls autoplay muted><source src="{{ site.baseurl }}/assets/img/bonus/w_interp3.mp4" type="video/mp4"></video> |


## BLIND

BLIND

Thank you for checking out our work!


<!-- **NOTE:**
Citations, footnotes, and code blocks do not display correctly in the dark mode since distill does not support the dark mode by default.
If you are interested in correctly adding dark mode support for distill, please open [a discussion](https://github.com/alshedivat/al-folio/discussions) and let us know. -->

<!-- ## Layouts

The main text column is referred to as the body.
It is the assumed layout of any direct descendants of the `d-article` element.

<div class="fake-img l-body">
  <p>.l-body</p>
</div>

For images you want to display a little larger, try `.l-page`:

<div class="fake-img l-page">
  <p>.l-page</p>
</div>

All of these have an outset variant if you want to poke out from the body text a little bit.
For instance:

<div class="fake-img l-body-outset">
  <p>.l-body-outset</p>
</div>

<div class="fake-img l-page-outset">
  <p>.l-page-outset</p>
</div>

Occasionally you’ll want to use the full browser width.
For this, use `.l-screen`.
You can also inset the element a little from the edge of the browser by using the inset variant.

<div class="fake-img l-screen">
  <p>.l-screen</p>
</div>
<div class="fake-img l-screen-inset">
  <p>.l-screen-inset</p>
</div>

The final layout is for marginalia, asides, and footnotes.
It does not interrupt the normal flow of `.l-body` sized text except on mobile screen sizes.

<div class="fake-img l-gutter">
  <p>.l-gutter</p>
</div>

*** -->

<!-- ## Other Typography? -->
<!-- 
Emphasis, aka italics, with *asterisks* (`*asterisks*`) or _underscores_ (`_underscores_`).

Strong emphasis, aka bold, with **asterisks** or __underscores__.

Combined emphasis with **asterisks and _underscores_**.

Strikethrough uses two tildes. ~~Scratch this.~~

1. First ordered list item
2. Another item
⋅⋅* Unordered sub-list. 
1. Actual numbers don't matter, just that it's a number
⋅⋅1. Ordered sub-list
4. And another item.

⋅⋅⋅You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we'll use three here to also align the raw Markdown).

⋅⋅⋅To have a line break without a paragraph, you will need to use two trailing spaces.⋅⋅
⋅⋅⋅Note that this line is separate, but within the same paragraph.⋅⋅
⋅⋅⋅(This is contrary to the typical GFM line break behaviour, where trailing spaces are not required.)

* Unordered list can use asterisks
- Or minuses
+ Or pluses

[I'm an inline-style link](https://www.google.com)

[I'm an inline-style link with title](https://www.google.com "Google's Homepage")

[I'm a reference-style link][Arbitrary case-insensitive reference text]

[I'm a relative reference to a repository file](../blob/master/LICENSE)

[You can use numbers for reference-style link definitions][1]

Or leave it empty and use the [link text itself].

URLs and URLs in angle brackets will automatically get turned into links. 
http://www.example.com or <http://www.example.com> and sometimes 
example.com (but not on Github, for example).

Some text to show that the reference links can follow later.

[arbitrary case-insensitive reference text]: https://www.mozilla.org
[1]: http://slashdot.org
[link text itself]: http://www.reddit.com

Here's our logo (hover to see the title text):

Inline-style: 
![alt text](https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png "Logo Title Text 1")

Reference-style: 
![alt text][logo]


Inline `code` has `back-ticks around` it.

```javascript
var s = "JavaScript syntax highlighting";
alert(s);
```
 
 
```
No language indicated, so no syntax highlighting. 
But let's throw in a <b>tag</b>.
```

Markdown | Less | Pretty
--- | --- | ---
*Still* | `renders` | **nicely**
1 | 2 | 3

> Blockquotes are very handy in email to emulate reply text.
> This line is part of the same quote.

Quote break.

> This is a very long line that will still be quoted properly when it wraps. Oh boy let's keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can *put* **Markdown** into a blockquote. 


Here's a line for us to start with.

This line is separated from the one above by two newlines, so it will be a *separate paragraph*.

This line is also a separate paragraph, but...
This line is only separated by a single newline, so it's a separate line in the *same paragraph*. -->
